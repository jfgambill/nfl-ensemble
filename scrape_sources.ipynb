{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# UPDATE THE WEEK BEFORE DOING ANYTHING ELSE!!!!!!!!!!!!!!\n",
    "#########################################################################\n",
    "\n",
    "#Which week of the NFl season are we in?\n",
    "week = 8\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE THE WEEK BEFORE DOING ANYTHING ELSE!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# Where is the chromedriver on this machine?\n",
    "####################################################################\n",
    "## change path as needed\n",
    "PATH_TO_CHROMEDRIVER = r'C:\\Users\\JoGa\\Documents\\PyScripts\\chromedriver' \n",
    "\n",
    "#####################################################################\n",
    "# Update the list of URLs as needed\n",
    "#####################################################################\n",
    "\n",
    "#swish analytics\n",
    "SWISH_URL = 'swishanalytics.com'\n",
    "SWISH_EXTENSION = '/optimus/nfl/daily-fantasy-projections'\n",
    "\n",
    "#CBS Sports\n",
    "CBS_URL = 'www.cbssports.com'\n",
    "CBS_EXTENSION = '/fantasy/football/stats/weeklyprojections/{pos}/{wk}/avg/standard?&print_rows=9999'\n",
    "\n",
    "#Fantasy Pros - (So far includes STATS,cbssports,espn,fftoday and numberFire)\n",
    "FP_URL = 'www.fantasypros.com'\n",
    "FP_EXTENSION = '/nfl/projections/{pos}.php?scoring=PPR'\n",
    "\n",
    "#Fantasy Sharks \n",
    "FS_URL = 'www.fantasysharks.com'\n",
    "FS_EXTENSION = '/apps/Projections/WeeklyProjections.php?pos=ALL&format=json'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########### Python 3 #############\n",
    "from bs4 import BeautifulSoup\n",
    "import http.client \n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "#For websites that have a different extension for each position\n",
    "POSITION_LIST = ['QB','RB','WR','TE','K','DST']\n",
    "\n",
    "def GetPage(url, extension, return_string=False):\n",
    "    # Takes a url and an extension and returns the page as a bs4.BeautifulSoup object\n",
    "    # or if return_string is set to True, it will return a string\n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection(url)\n",
    "        conn.request(\"GET\", extension)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "        data = data.decode('utf-8')\n",
    "        conn.close()\n",
    "        soup = BeautifulSoup(data, 'lxml')\n",
    "        if return_string:\n",
    "            return str(soup)\n",
    "        else:\n",
    "            return soup\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "    \n",
    "    \n",
    "#connect to database\n",
    "engine = create_engine('sqlite:///nfl_projections_db.sqlite')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projections for week 8 have been successfully loaded into swish_week_8\n"
     ]
    }
   ],
   "source": [
    "# Get projections from Swish Analytics\n",
    "# point the urls to swish\n",
    "url = SWISH_URL\n",
    "extension = SWISH_EXTENSION\n",
    "table_name = 'swish_week_{}'.format(week)\n",
    "\n",
    "# Convert the webpage to a string\n",
    "soup = GetPage(url, extension, return_string=True)\n",
    "\n",
    "# Check to make sure that the page was retrieved\n",
    "assert len(soup) > 0, print(\"The content at {} was not retrieved\".format(url))\n",
    "\n",
    "# Find the start of the projections which begins with this string: \"this.players = \"\n",
    "start = soup.find('this.players = ') + len('this.players = ')\n",
    "\n",
    "#find the end projections in the string\n",
    "end = soup.find('\"}];') + len('\"}]')\n",
    "\n",
    "# slice the string down to the projections\n",
    "projections = soup[start:end]\n",
    "# Check to make sure that there are projections on the page\n",
    "assert len(projections) > 0, print(\"There appear to be no projections at {}\".format(url))\n",
    "    \n",
    "try:\n",
    "    #Convert the string to a dataframe\n",
    "    swish_projections = json_normalize(json.loads(projections))\n",
    "except JSONDecodeError:\n",
    "    print(\"Error converting the projections from {} into json\".format(url))\n",
    "    \n",
    "#rename columns\n",
    "swish_projections.columns = ['avg_yards', 'avg_tds', 'proj_yds', 'proj_tds', 'date', 'dk_avg',\n",
    "       'dk_fpts', 'dk_fpts_act', 'dk_fpts_ingame', 'dk_salary', 'dk_value',\n",
    "       'event_status_id', 'fd_avg', 'fd_fpts', 'fd_fpts_act', 'fd_fpts_ingame',\n",
    "       'fd_salary', 'fd_value', 'home', 'name', 'nfl_avg', 'nfl_fpts',\n",
    "       'nfl_fpts_act', 'nfl_fpts_ingame', 'nfl_salary', 'nfl_value',\n",
    "       'nickname', 'opp_abbr', 'player_id', 'primary_pos_abbr',\n",
    "       'season_fpts_full', 'season_fpts_remaining', 'ya_avg', 'ya_fpts',\n",
    "       'ya_fpts_act', 'ya_fpts_ingame', 'ya_salary', 'ya_value']   \n",
    "\n",
    "# Change data types to numeric if possible, ignore if text\n",
    "swish_projections = swish_projections.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "swish_projections[['dk_salary','fd_salary']] = swish_projections[['dk_salary','fd_salary']].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "\n",
    "try:    \n",
    "    #load projections to a sql table\n",
    "    swish_projections.to_sql(table_name, engine, if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"Projections for week {} have been successfully loaded into {}\".format(week, table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBS projections have been successfully saved to these tables: ['cbs_qb_week_8', 'cbs_rb_week_8', 'cbs_wr_week_8', 'cbs_te_week_8', 'cbs_k_week_8', 'cbs_dst_week_8']\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# Get projections from CBS\n",
    "#################################################\n",
    "\n",
    "# point the urls to CBS\n",
    "url = CBS_URL\n",
    "extension = CBS_EXTENSION\n",
    "\n",
    "#set up empty df to append the dfs for each position to \n",
    "cbs_df = pd.DataFrame()\n",
    "\n",
    "#create an empty list of tables to append to\n",
    "table_list = [] \n",
    "\n",
    "# for each position scrape the different extension\n",
    "for position in POSITION_LIST:\n",
    "    soup = GetPage(url, extension.format(pos=position, wk=week))\n",
    "    assert len(soup) > 0, print(\"Problem retrieving {} {}\".format(url, extension.format(pos=position, wk=week)))\n",
    "    \n",
    "    sql_table_name = 'cbs_{}_week_{}'.format(position.lower(), week)\n",
    "    table = soup.find('table')\n",
    "    assert len(table) > 0, print(\"Problem finding table on {} {}\".format(url, extension.format(pos=position, wk=week)))\n",
    "    \n",
    "    if position in ['QB','RB','WR','TE']:\n",
    "        #The header is in row 2 and the last row needs to be dropped\n",
    "        header_row = 2\n",
    "        drop_last_row = True\n",
    "    else:\n",
    "        header_row = 1\n",
    "        drop_last_row = False\n",
    "        \n",
    "    df = pd.read_html(str(table),header=header_row)[0]\n",
    "    assert len(df) > 0, print(\"Problem reading {} html table\".format(position))\n",
    "    \n",
    "    if drop_last_row:\n",
    "        df = df.ix[:len(df)-2]\n",
    "    try:    \n",
    "        df.to_sql(sql_table_name, engine, if_exists='replace')\n",
    "        table_list.append(sql_table_name)\n",
    "    except Exception as e:    \n",
    "        print(e)\n",
    "      \n",
    "    \n",
    "print(\"CBS projections have been successfully saved to these tables: {}\".format(table_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projections for Fantasy Pros have been added to these tables: ['fantasy_pros_qb_week_8', 'fantasy_pros_rb_week_8', 'fantasy_pros_wr_week_8', 'fantasy_pros_te_week_8', 'fantasy_pros_k_week_8', 'fantasy_pros_dst_week_8']\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Get projections from Fantasy Pros\n",
    "################################################\n",
    "\n",
    "# point the urls to Fantasy Pros\n",
    "url = FP_URL\n",
    "extension = FP_EXTENSION\n",
    "\n",
    "table_list = []\n",
    "\n",
    "# for each position scrape the different extension\n",
    "for position in POSITION_LIST:\n",
    "    soup = GetPage(url, extension.format(pos=position.lower()))\n",
    "    assert len(soup) > 0, print(\"Problem retrieving {}{}\".format(url, extension.format(pos=position, wk=week)))\n",
    "    \n",
    "    sql_table_name = 'fantasy_pros_{}_week_{}'.format(position.lower(), week)\n",
    "    table = soup.find('table')\n",
    "    assert len(table) > 0, print(\"Problem finding table on {}{}\".format(url, extension.format(pos=position, wk=week)))\n",
    "    \n",
    "    df = pd.read_html(str(table))[0]\n",
    "    assert len(df) > 0, print(\"Problem reading {} html table\".format(position))\n",
    "    \n",
    "        \n",
    "    if position != 'DST':\n",
    "        names = []\n",
    "        teams = []\n",
    "        for i in df['Player'].iteritems():\n",
    "            s = i[1]\n",
    "            space = s[len(s)::-1].find(' ')\n",
    "            name = s[:len(s) - space - 1 ]\n",
    "            team = s[len(s) - space :len(s)]\n",
    "            names.append(name)\n",
    "            teams.append(team)    \n",
    "        df['Player'], df['Team'] = names, teams\n",
    "        \n",
    "    col_list = []\n",
    "    for col in df.columns:\n",
    "        if col[len(col) - 2] == '.':\n",
    "            new_col = str(col[:len(col) - 2]) + 'z'\n",
    "            col_list.append(new_col)\n",
    "        else:\n",
    "            col_list.append(col)\n",
    "    df.columns = col_list\n",
    "    \n",
    "    try: \n",
    "        df.to_sql(sql_table_name, engine, if_exists='replace')\n",
    "        table_list.append(sql_table_name)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "print(\"Projections for Fantasy Pros have been added to these tables: {}\".format(table_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fantasy Sharks projections have been successfully added to fantasy_sharks_week_8\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Get projections from Fantasy Sharks\n",
    "################################################\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import clipboard\n",
    "\n",
    "# The Fantasy Sharks Website apparently doesn't like to be scraped. Need to use selenium webdriver\n",
    "\n",
    "# point the urls to Fantasy Sharks\n",
    "url = 'http://' + FS_URL + FS_EXTENSION\n",
    "table_name = 'fantasy_sharks_week_{}'.format(week)\n",
    "\n",
    "# Set modified User-Agent string so that website thinks we're not using a \n",
    "# selenium browser\n",
    "dcap = {}\n",
    "dcap[\"phantomjs.page.settings.userAgent\"] = (\n",
    "     \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/53 \"\n",
    "     \"(KHTML, like Gecko) Chrome/15.0.87\")\n",
    "\n",
    "#use the selenium webdriver to connect to Chrome\n",
    "browser = webdriver.Chrome(executable_path = PATH_TO_CHROMEDRIVER,\n",
    "                          desired_capabilities = dcap)\n",
    "\n",
    "#connect to fantasy sharks\n",
    "browser.get(url)\n",
    "\n",
    "#the page is just a json file. Selenium (I don't think) has any function that will grab it.\n",
    "# Need to send keyboard commands to select all text and copy to clipboard.\n",
    "actions = ActionChains(browser)\n",
    "actions.key_down(Keys.CONTROL).send_keys('a').key_up(Keys.CONTROL).perform()\n",
    "actions.key_down(Keys.CONTROL).send_keys('c').key_up(Keys.CONTROL).perform()\n",
    "browser.quit()\n",
    "\n",
    "# Get contents of clipboard\n",
    "text = clipboard.paste()\n",
    "assert len(text) > 0, print(\"There was a problem with downloading from {}.\".format(url))\n",
    "\n",
    "# Convert text to DataFrame\n",
    "fs_projections = json_normalize(json.loads(text))\n",
    "assert len(fs_projections) > 0, print(\"The text could not be converted to a DataFrame.\")\n",
    "\n",
    "fs_projections = fs_projections.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "\n",
    "try:\n",
    "    fs_projections.to_sql('fantasy_sharks_week_{}'.format(week), engine,if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:    \n",
    "    print(\"Fantasy Sharks projections have been successfully added to {}\".format(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fantasy Data projections have been successfully added to fantasy_data_def_week_8\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#  Get Defensive Stats from Fantasy Data\n",
    "###########################################################\n",
    "\n",
    "headers = {\n",
    "    # Request headers\n",
    "    'Ocp-Apim-Subscription-Key': '<your_api_key_here>',\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection('api.fantasydata.net')\n",
    "    conn.request(\"GET\", \"/v3/nfl/projections/JSON/FantasyDefenseProjectionsByGame/2016REG/{wk}\".format(wk=week),\\\n",
    "                 \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    data = data.decode('utf-8')\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n",
    "####################################\n",
    "\n",
    "table_name = 'fantasy_data_def_week_{}'.format(week)\n",
    "\n",
    "try:\n",
    "    fd_def_stats = json_normalize(json.loads(data))\n",
    "    fd_def_stats.to_sql(table_name, engine, if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:    \n",
    "    print(\"Fantasy Data projections have been successfully added to {}\".format(table_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fantasy Data projections have been successfully added to fantasy_data_off_week_8\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#  Get Offensive Stats from Fantasy Data\n",
    "###########################################################\n",
    "\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection('api.fantasydata.net')\n",
    "    conn.request(\"GET\", \"/v3/nfl/projections/JSON/PlayerGameProjectionStatsByWeek/2016REG/{wk}\".format(wk=week),\\\n",
    "                 \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    data = data.decode('utf-8')\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n",
    "####################################\n",
    "\n",
    "table_name = 'fantasy_data_off_week_{}'.format(week)\n",
    "\n",
    "try:\n",
    "    fd_plyr_stats = json_normalize(json.loads(data))\n",
    "    fd_plyr_stats.to_sql(table_name, engine, if_exists='replace')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:    \n",
    "    print(\"Fantasy Data projections have been successfully added to {}\".format(table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
